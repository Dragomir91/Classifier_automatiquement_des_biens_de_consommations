{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4c0fb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f5d69ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11a7dde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import tensorflow_hub as hub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "import time\n",
    "%matplotlib inline\n",
    "import os\n",
    "from os import listdir\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, auc, roc_auc_score, roc_curve, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfb474c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a4e6cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Source/flipkart_com-ecommerce_sample_1050.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42bbd83b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniq_id</th>\n",
       "      <th>crawl_timestamp</th>\n",
       "      <th>product_url</th>\n",
       "      <th>product_name</th>\n",
       "      <th>product_category_tree</th>\n",
       "      <th>pid</th>\n",
       "      <th>retail_price</th>\n",
       "      <th>discounted_price</th>\n",
       "      <th>image</th>\n",
       "      <th>is_FK_Advantage_product</th>\n",
       "      <th>description</th>\n",
       "      <th>product_rating</th>\n",
       "      <th>overall_rating</th>\n",
       "      <th>brand</th>\n",
       "      <th>product_specifications</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55b85ea15a1536d46b7190ad6fff8ce7</td>\n",
       "      <td>2016-04-30 03:22:56 +0000</td>\n",
       "      <td>http://www.flipkart.com/elegance-polyester-mul...</td>\n",
       "      <td>Elegance Polyester Multicolor Abstract Eyelet ...</td>\n",
       "      <td>[\"Home Furnishing &gt;&gt; Curtains &amp; Accessories &gt;&gt;...</td>\n",
       "      <td>CRNEG7BKMFFYHQ8Z</td>\n",
       "      <td>1899.0</td>\n",
       "      <td>899.0</td>\n",
       "      <td>55b85ea15a1536d46b7190ad6fff8ce7.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>Key Features of Elegance Polyester Multicolor ...</td>\n",
       "      <td>No rating available</td>\n",
       "      <td>No rating available</td>\n",
       "      <td>Elegance</td>\n",
       "      <td>{\"product_specification\"=&gt;[{\"key\"=&gt;\"Brand\", \"v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7b72c92c2f6c40268628ec5f14c6d590</td>\n",
       "      <td>2016-04-30 03:22:56 +0000</td>\n",
       "      <td>http://www.flipkart.com/sathiyas-cotton-bath-t...</td>\n",
       "      <td>Sathiyas Cotton Bath Towel</td>\n",
       "      <td>[\"Baby Care &gt;&gt; Baby Bath &amp; Skin &gt;&gt; Baby Bath T...</td>\n",
       "      <td>BTWEGFZHGBXPHZUH</td>\n",
       "      <td>600.0</td>\n",
       "      <td>449.0</td>\n",
       "      <td>7b72c92c2f6c40268628ec5f14c6d590.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>Specifications of Sathiyas Cotton Bath Towel (...</td>\n",
       "      <td>No rating available</td>\n",
       "      <td>No rating available</td>\n",
       "      <td>Sathiyas</td>\n",
       "      <td>{\"product_specification\"=&gt;[{\"key\"=&gt;\"Machine Wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64d5d4a258243731dc7bbb1eef49ad74</td>\n",
       "      <td>2016-04-30 03:22:56 +0000</td>\n",
       "      <td>http://www.flipkart.com/eurospa-cotton-terry-f...</td>\n",
       "      <td>Eurospa Cotton Terry Face Towel Set</td>\n",
       "      <td>[\"Baby Care &gt;&gt; Baby Bath &amp; Skin &gt;&gt; Baby Bath T...</td>\n",
       "      <td>BTWEG6SHXTDB2A2Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64d5d4a258243731dc7bbb1eef49ad74.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>Key Features of Eurospa Cotton Terry Face Towe...</td>\n",
       "      <td>No rating available</td>\n",
       "      <td>No rating available</td>\n",
       "      <td>Eurospa</td>\n",
       "      <td>{\"product_specification\"=&gt;[{\"key\"=&gt;\"Material\",...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            uniq_id            crawl_timestamp  \\\n",
       "0  55b85ea15a1536d46b7190ad6fff8ce7  2016-04-30 03:22:56 +0000   \n",
       "1  7b72c92c2f6c40268628ec5f14c6d590  2016-04-30 03:22:56 +0000   \n",
       "2  64d5d4a258243731dc7bbb1eef49ad74  2016-04-30 03:22:56 +0000   \n",
       "\n",
       "                                         product_url  \\\n",
       "0  http://www.flipkart.com/elegance-polyester-mul...   \n",
       "1  http://www.flipkart.com/sathiyas-cotton-bath-t...   \n",
       "2  http://www.flipkart.com/eurospa-cotton-terry-f...   \n",
       "\n",
       "                                        product_name  \\\n",
       "0  Elegance Polyester Multicolor Abstract Eyelet ...   \n",
       "1                         Sathiyas Cotton Bath Towel   \n",
       "2                Eurospa Cotton Terry Face Towel Set   \n",
       "\n",
       "                               product_category_tree               pid  \\\n",
       "0  [\"Home Furnishing >> Curtains & Accessories >>...  CRNEG7BKMFFYHQ8Z   \n",
       "1  [\"Baby Care >> Baby Bath & Skin >> Baby Bath T...  BTWEGFZHGBXPHZUH   \n",
       "2  [\"Baby Care >> Baby Bath & Skin >> Baby Bath T...  BTWEG6SHXTDB2A2Y   \n",
       "\n",
       "   retail_price  discounted_price                                 image  \\\n",
       "0        1899.0             899.0  55b85ea15a1536d46b7190ad6fff8ce7.jpg   \n",
       "1         600.0             449.0  7b72c92c2f6c40268628ec5f14c6d590.jpg   \n",
       "2           NaN               NaN  64d5d4a258243731dc7bbb1eef49ad74.jpg   \n",
       "\n",
       "   is_FK_Advantage_product                                        description  \\\n",
       "0                    False  Key Features of Elegance Polyester Multicolor ...   \n",
       "1                    False  Specifications of Sathiyas Cotton Bath Towel (...   \n",
       "2                    False  Key Features of Eurospa Cotton Terry Face Towe...   \n",
       "\n",
       "        product_rating       overall_rating     brand  \\\n",
       "0  No rating available  No rating available  Elegance   \n",
       "1  No rating available  No rating available  Sathiyas   \n",
       "2  No rating available  No rating available   Eurospa   \n",
       "\n",
       "                              product_specifications  \n",
       "0  {\"product_specification\"=>[{\"key\"=>\"Brand\", \"v...  \n",
       "1  {\"product_specification\"=>[{\"key\"=>\"Machine Wa...  \n",
       "2  {\"product_specification\"=>[{\"key\"=>\"Material\",...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "161c2a92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>product_category_tree</th>\n",
       "      <th>description</th>\n",
       "      <th>brand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Elegance Polyester Multicolor Abstract Eyelet ...</td>\n",
       "      <td>[\"Home Furnishing &gt;&gt; Curtains &amp; Accessories &gt;&gt;...</td>\n",
       "      <td>Key Features of Elegance Polyester Multicolor ...</td>\n",
       "      <td>Elegance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sathiyas Cotton Bath Towel</td>\n",
       "      <td>[\"Baby Care &gt;&gt; Baby Bath &amp; Skin &gt;&gt; Baby Bath T...</td>\n",
       "      <td>Specifications of Sathiyas Cotton Bath Towel (...</td>\n",
       "      <td>Sathiyas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Eurospa Cotton Terry Face Towel Set</td>\n",
       "      <td>[\"Baby Care &gt;&gt; Baby Bath &amp; Skin &gt;&gt; Baby Bath T...</td>\n",
       "      <td>Key Features of Eurospa Cotton Terry Face Towe...</td>\n",
       "      <td>Eurospa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SANTOSH ROYAL FASHION Cotton Printed King size...</td>\n",
       "      <td>[\"Home Furnishing &gt;&gt; Bed Linen &gt;&gt; Bedsheets &gt;&gt;...</td>\n",
       "      <td>Key Features of SANTOSH ROYAL FASHION Cotton P...</td>\n",
       "      <td>SANTOSH ROYAL FASHION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jaipur Print Cotton Floral King sized Double B...</td>\n",
       "      <td>[\"Home Furnishing &gt;&gt; Bed Linen &gt;&gt; Bedsheets &gt;&gt;...</td>\n",
       "      <td>Key Features of Jaipur Print Cotton Floral Kin...</td>\n",
       "      <td>Jaipur Print</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>Oren Empower Extra Large Self Adhesive Sticker</td>\n",
       "      <td>[\"Baby Care &gt;&gt; Baby &amp; Kids Gifts &gt;&gt; Stickers &gt;...</td>\n",
       "      <td>Oren Empower Extra Large Self Adhesive Sticker...</td>\n",
       "      <td>Oren Empower</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046</th>\n",
       "      <td>Wallmantra Large Vinyl Sticker Sticker</td>\n",
       "      <td>[\"Baby Care &gt;&gt; Baby &amp; Kids Gifts &gt;&gt; Stickers &gt;...</td>\n",
       "      <td>Wallmantra Large Vinyl Sticker Sticker (Pack o...</td>\n",
       "      <td>Wallmantra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047</th>\n",
       "      <td>Uberlyfe Extra Large Pigmented Polyvinyl Films...</td>\n",
       "      <td>[\"Baby Care &gt;&gt; Baby &amp; Kids Gifts &gt;&gt; Stickers &gt;...</td>\n",
       "      <td>Buy Uberlyfe Extra Large Pigmented Polyvinyl F...</td>\n",
       "      <td>Uberlyfe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048</th>\n",
       "      <td>Wallmantra Medium Vinyl Sticker Sticker</td>\n",
       "      <td>[\"Baby Care &gt;&gt; Baby &amp; Kids Gifts &gt;&gt; Stickers &gt;...</td>\n",
       "      <td>Buy Wallmantra Medium Vinyl Sticker Sticker fo...</td>\n",
       "      <td>Wallmantra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049</th>\n",
       "      <td>Uberlyfe Large Vinyl Sticker</td>\n",
       "      <td>[\"Baby Care &gt;&gt; Baby &amp; Kids Gifts &gt;&gt; Stickers &gt;...</td>\n",
       "      <td>Buy Uberlyfe Large Vinyl Sticker for Rs.595 on...</td>\n",
       "      <td>Uberlyfe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1050 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           product_name  \\\n",
       "0     Elegance Polyester Multicolor Abstract Eyelet ...   \n",
       "1                            Sathiyas Cotton Bath Towel   \n",
       "2                   Eurospa Cotton Terry Face Towel Set   \n",
       "3     SANTOSH ROYAL FASHION Cotton Printed King size...   \n",
       "4     Jaipur Print Cotton Floral King sized Double B...   \n",
       "...                                                 ...   \n",
       "1045     Oren Empower Extra Large Self Adhesive Sticker   \n",
       "1046             Wallmantra Large Vinyl Sticker Sticker   \n",
       "1047  Uberlyfe Extra Large Pigmented Polyvinyl Films...   \n",
       "1048            Wallmantra Medium Vinyl Sticker Sticker   \n",
       "1049                       Uberlyfe Large Vinyl Sticker   \n",
       "\n",
       "                                  product_category_tree  \\\n",
       "0     [\"Home Furnishing >> Curtains & Accessories >>...   \n",
       "1     [\"Baby Care >> Baby Bath & Skin >> Baby Bath T...   \n",
       "2     [\"Baby Care >> Baby Bath & Skin >> Baby Bath T...   \n",
       "3     [\"Home Furnishing >> Bed Linen >> Bedsheets >>...   \n",
       "4     [\"Home Furnishing >> Bed Linen >> Bedsheets >>...   \n",
       "...                                                 ...   \n",
       "1045  [\"Baby Care >> Baby & Kids Gifts >> Stickers >...   \n",
       "1046  [\"Baby Care >> Baby & Kids Gifts >> Stickers >...   \n",
       "1047  [\"Baby Care >> Baby & Kids Gifts >> Stickers >...   \n",
       "1048  [\"Baby Care >> Baby & Kids Gifts >> Stickers >...   \n",
       "1049  [\"Baby Care >> Baby & Kids Gifts >> Stickers >...   \n",
       "\n",
       "                                            description                  brand  \n",
       "0     Key Features of Elegance Polyester Multicolor ...               Elegance  \n",
       "1     Specifications of Sathiyas Cotton Bath Towel (...               Sathiyas  \n",
       "2     Key Features of Eurospa Cotton Terry Face Towe...                Eurospa  \n",
       "3     Key Features of SANTOSH ROYAL FASHION Cotton P...  SANTOSH ROYAL FASHION  \n",
       "4     Key Features of Jaipur Print Cotton Floral Kin...           Jaipur Print  \n",
       "...                                                 ...                    ...  \n",
       "1045  Oren Empower Extra Large Self Adhesive Sticker...           Oren Empower  \n",
       "1046  Wallmantra Large Vinyl Sticker Sticker (Pack o...             Wallmantra  \n",
       "1047  Buy Uberlyfe Extra Large Pigmented Polyvinyl F...               Uberlyfe  \n",
       "1048  Buy Wallmantra Medium Vinyl Sticker Sticker fo...             Wallmantra  \n",
       "1049  Buy Uberlyfe Large Vinyl Sticker for Rs.595 on...               Uberlyfe  \n",
       "\n",
       "[1050 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# premier notebook NLP 40 % \n",
    "# deux parties : texte (données non structurées)\n",
    "# deuxième notebook image 40 % \n",
    "\n",
    "cols = ['product_name',\n",
    "        'product_category_tree',\n",
    "        'description',\n",
    "        'brand']\n",
    "\n",
    "df_texte = df.loc[:,cols]\n",
    "df_texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a52f9eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dragomir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# lire des caractère sur une image\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "token = np.array(df_texte.product_category_tree)\n",
    "\n",
    "token = [tokenizer.tokenize(i.lower()) for i in token]\n",
    "\n",
    "bigram = [list(nltk.bigrams(bigram)) for bigram in token]\n",
    "\n",
    "token_bigram = pd.Series(bigram)\n",
    "\n",
    "cat_name = np.array([token_bigram[i][0] for i in range(len(token_bigram))])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "730fa529",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_token = pd.DataFrame(cat_name, columns=['cat1','cat2'])\n",
    "df_token = df_token.cat1 + ' ' +  df_token.cat2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8542565",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_texte['category_name'] = df_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb1abc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_texte.loc[df_texte.category_name == 'computers laptops','category_name'] = 'computers laptop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13b795d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "token2 = np.array(df_texte.description)\n",
    "token2 = [tokenizer.tokenize(i.lower()) for i in token2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d6a92b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dragomir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62d5905c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = set()\n",
    "sw.update(tuple(nltk.corpus.stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e1d1b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "descript_sw = [[idx for idx in token if idx not in sw] for token in token2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6ac1696",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import EnglishStemmer\n",
    "\n",
    "stemmer = EnglishStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7e0a333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['key', 'features', 'elegance', 'polyester', 'multicolor', 'abstract', 'eyelet', 'door', 'curtain', \""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_stem = [stemmer.stem(str(i)) for i in descript_sw]\n",
    "\n",
    "data_stem[0][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "771f4ba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"['key', 'features', 'elegance', 'polyester', 'multicolor', 'abstract', 'eyelet', 'door', 'curtain', 'floral', 'curtain', 'elegance', 'polyester', 'multicolor', 'abstract', 'eyelet', 'door', 'curtain', '213', 'cm', 'height', 'pack', '2', 'price', 'rs', '899', 'curtain', 'enhances', 'look', 'interiors', 'curtain', 'made', '100', 'high', 'quality', 'polyester', 'fabric', 'features', 'eyelet', 'style', 'stitch', 'metal', 'ring', 'makes', 'room', 'environment', 'romantic', 'loving', 'curtain', 'ant', 'wrinkle', 'anti', 'shrinkage', 'elegant', 'apparance', 'give', 'home', 'bright', 'modernistic', 'appeal', 'designs', 'surreal', 'attention', 'sure', 'steal', 'hearts', 'contemporary', 'eyelet', 'valance', 'curtains', 'slide', 'smoothly', 'draw', 'apart', 'first', 'thing', 'morning', 'welcome', 'bright', 'sun', 'rays', 'want', 'wish', 'good', 'morning', 'whole', 'world', 'draw', 'close', 'evening', 'create', 'special', 'moments', 'joyous', 'beauty', 'given', 'soothing', 'prints', 'bring', 'home', 'elegant', 'curtain', 'softly', 'filters', 'light', 'room', 'get', 'right', 'amount', 'sunlight', 'specifications', 'elegance', 'polyester', 'multicolor', 'abstract', 'eyelet', 'door', 'curtain', '213', 'cm', 'height', 'pack', '2', 'general', 'brand', 'elegance', 'designed', 'door', 'type', 'eyelet', 'model', 'name', 'abstract', 'polyester', 'door', 'curtain', 'set', '2', 'model', 'id', 'duster25', 'color', 'multicolor', 'dimensions', 'length', '213', 'cm', 'box', 'number', 'contents', 'sales', 'package', 'pack', '2', 'sales', 'package', '2', 'curtains', 'body', 'design', 'material', 'polyester']\",\n",
       " \"['specifications', 'sathiyas', 'cotton', 'bath', 'towel', '3', 'bath', 'towel', 'red', 'yellow', 'blue', 'bath', 'towel', 'features', 'machine', 'washable', 'yes', 'material', 'cotton', 'design', 'self', 'design', 'general', 'brand', 'sathiyas', 'type', 'bath', 'towel', 'gsm', '500', 'model', 'name', 'sathiyas', 'cotton', 'bath', 'towel', 'ideal', 'men', 'women', 'boys', 'girls', 'model', 'id', 'asvtwl322', 'color', 'red', 'yellow', 'blue', 'size', 'mediam', 'dimensions', 'length', '30', 'inch', 'width', '60', 'inch', 'box', 'number', 'contents', 'sales', 'package', '3', 'sales', 'package', '3', 'bath', 'towel']\"]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Stem_word = [''.join(stemword) for stemword in data_stem]\n",
    "Stem_word[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e5b966",
   "metadata": {},
   "source": [
    "### Word embeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e0dae0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "602ddd5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "home furnishing       150\n",
       "baby care             150\n",
       "home decor            150\n",
       "kitchen dining        150\n",
       "beauty and            150\n",
       "watches wrist         149\n",
       "computers laptop       89\n",
       "computers network      49\n",
       "computers computer      6\n",
       "computers tablet        3\n",
       "computers storage       2\n",
       "computers software      1\n",
       "watches clocks          1\n",
       "Name: category_name, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "# preprocessing encodage des données\n",
    "encodage = preprocessing.LabelEncoder()\n",
    "encodage.fit(df_texte.category_name)\n",
    "df_texte[\"label_cat1\"] = encodage.transform(df_texte.category_name)\n",
    "# data.head(5)\n",
    "df_texte.category_name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "817923db",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stem_word2 = [tokenizer.tokenize(i.lower()) for i in Stem_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "28d34899",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dragomir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Dragomir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'curtain'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "Lemmatise_corpus = [[lemmatizer.lemmatize(Lemma) for Lemma in Stem_word2[idx]] for idx in range(len(Stem_word2))]\n",
    "\n",
    "Lemmatise_corpus[0][10]\n",
    "                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4deb37ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(Lemmatise_corpus, min_count=1,hs=1,negative=0,sg=0,epochs=5)  # min_count spécifie la fréquence minimale d'un mot pour être inclus dans le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6cf3daa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gifting', 0.7993993163108826),\n",
       " ('crafted', 0.794390857219696),\n",
       " ('look', 0.7491447329521179),\n",
       " ('express', 0.7399798631668091),\n",
       " ('one', 0.7333756685256958),\n",
       " ('permanat', 0.7186675071716309),\n",
       " ('pic', 0.716445803642273),\n",
       " ('wood', 0.7043812274932861),\n",
       " ('gift', 0.6971081495285034),\n",
       " ('help', 0.693030834197998)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('home')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34d1dc95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gifting', 0.7993993163108826),\n",
       " ('crafted', 0.794390857219696),\n",
       " ('look', 0.7491447329521179),\n",
       " ('express', 0.7399798631668091),\n",
       " ('one', 0.7333756685256958),\n",
       " ('permanat', 0.7186675071716309),\n",
       " ('pic', 0.716445803642273),\n",
       " ('wood', 0.7043812274932861),\n",
       " ('gift', 0.6971081495285034),\n",
       " ('help', 0.693030834197998)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('home')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c571663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['home', 'furnishing'],\n",
       " ['baby', 'care'],\n",
       " ['watches', 'wrist'],\n",
       " ['home', 'decor'],\n",
       " ['kitchen', 'dining'],\n",
       " ['beauty', 'and'],\n",
       " ['computers', 'network'],\n",
       " ['computers', 'tablet'],\n",
       " ['computers', 'laptop'],\n",
       " ['computers', 'software'],\n",
       " ['computers', 'computer'],\n",
       " ['computers', 'storage'],\n",
       " ['watches', 'clocks']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_name = [tokenizer.tokenize(i) for i in df_texte.category_name.unique()]\n",
    "category_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fada6bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = Lemmatise_corpus\n",
    "\n",
    "# Initialize an empty array to store document vectors\n",
    "document_vectors = np.zeros((len(documents), model.vector_size))\n",
    "tab = []\n",
    "save = [tab.append(len(doc)) for doc in documents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b4de6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# récupére tous les vecteurs \n",
    "def wvector(corpus,model):\n",
    "    # List of documents\n",
    "    documents = corpus\n",
    "\n",
    "    # Initialize an empty array to store document vectors\n",
    "    document_vectors = np.zeros((len(documents), model.vector_size))\n",
    "    # Iterate over each document\n",
    "\n",
    "    for i, doc in enumerate(documents):\n",
    "        # Split the document into individual words\n",
    "        words = doc\n",
    "        # Initialize an empty array to store word vectors\n",
    "        word_vectors = np.zeros((len(words), model.vector_size))\n",
    "\n",
    "        #document_vectors = np.zeros((len(documents), model.vector_size))\n",
    "\n",
    "        # Iterate over each word in the document\n",
    "        for j, word in enumerate(words):\n",
    "            # Check if the word is in the vocabulary of the model\n",
    "            if word in model.wv.index_to_key:\n",
    "                # Get the word vector\n",
    "                word_vectors[j] = model.wv[word]\n",
    "        # Calculate the average vector of the document\n",
    "        if len(word_vectors) > 0:\n",
    "            #print(word_vectors.shape)\n",
    "            document_vectors[i] = np.mean(word_vectors, axis=0)\n",
    "    return document_vectors\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dda82a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "607d3b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline_evaluate_model(index_fold, X_train, X_test, y_train, y_test,idx_train,idx_test):\n",
    "    \n",
    "        model = Pipeline([\n",
    "                         (\"classifier\",SVC())\n",
    "                         ])\n",
    "\n",
    "        model.fit(X_train,y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        score = model.score(X_test,y_test)     \n",
    "        print(f\"Run {index_fold} : score = {round(score,2)} \")\n",
    "        return (score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a9f74af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 0 : score = 0.09 \n",
      "Run 1 : score = 0.01 \n",
      "Run 2 : score = 0.04 \n",
      "Run 3 : score = 0.07 \n",
      "Run 4 : score = 0.0 \n",
      "0.04095238095238095\n"
     ]
    }
   ],
   "source": [
    "nb_model = 5\n",
    "index_fold = 0\n",
    "kf = KFold(n_splits=nb_model, shuffle=False)\n",
    "average_score = 0\n",
    "\n",
    "X = document_vectors\n",
    "y = df_texte.label_cat1\n",
    "\n",
    "kf = KFold(n_splits=nb_model)\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    \n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    score = create_pipeline_evaluate_model(index_fold, X_train, X_test, y_train, y_test,train_index,test_index)    \n",
    "    average_score += score\n",
    "    index_fold += 1\n",
    "    \n",
    "average_score = average_score / nb_model\n",
    "print(average_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "80d587d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 5 : score = 0.1 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "y = df_texte.label_cat1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(document_vectors, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = Pipeline([\n",
    "                 (\"classifier\",SVC())\n",
    "                 ])\n",
    "\n",
    "model.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "score = model.score(X_test,y_test)     \n",
    "print(f\"Run {index_fold} : score = {round(score,2)} \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "107f6e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 9 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dragomir\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_estimator</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.658081</td>\n",
       "      <td>0.111044</td>\n",
       "      <td>0.040453</td>\n",
       "      <td>0.014389</td>\n",
       "      <td>ExtraTreesClassifier()</td>\n",
       "      <td>{'estimator': ExtraTreesClassifier()}</td>\n",
       "      <td>0.837143</td>\n",
       "      <td>0.094612</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.106299</td>\n",
       "      <td>0.020809</td>\n",
       "      <td>0.034401</td>\n",
       "      <td>0.011316</td>\n",
       "      <td>SVC()</td>\n",
       "      <td>{'estimator': SVC()}</td>\n",
       "      <td>0.826667</td>\n",
       "      <td>0.087991</td>\n",
       "      <td>2</td>\n",
       "      <td>0.873333</td>\n",
       "      <td>0.005334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.255159</td>\n",
       "      <td>0.177889</td>\n",
       "      <td>0.025101</td>\n",
       "      <td>0.005320</td>\n",
       "      <td>RandomForestClassifier()</td>\n",
       "      <td>{'estimator': RandomForestClassifier()}</td>\n",
       "      <td>0.825714</td>\n",
       "      <td>0.102402</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.194197</td>\n",
       "      <td>0.025820</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001095</td>\n",
       "      <td>LogisticRegression()</td>\n",
       "      <td>{'estimator': LogisticRegression()}</td>\n",
       "      <td>0.819048</td>\n",
       "      <td>0.097497</td>\n",
       "      <td>4</td>\n",
       "      <td>0.858519</td>\n",
       "      <td>0.007719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55.622946</td>\n",
       "      <td>1.858243</td>\n",
       "      <td>0.004004</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>GradientBoostingClassifier()</td>\n",
       "      <td>{'estimator': GradientBoostingClassifier()}</td>\n",
       "      <td>0.799048</td>\n",
       "      <td>0.100831</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.913551</td>\n",
       "      <td>0.079286</td>\n",
       "      <td>0.004001</td>\n",
       "      <td>0.000448</td>\n",
       "      <td>BaggingClassifier()</td>\n",
       "      <td>{'estimator': BaggingClassifier()}</td>\n",
       "      <td>0.795238</td>\n",
       "      <td>0.102685</td>\n",
       "      <td>6</td>\n",
       "      <td>0.993333</td>\n",
       "      <td>0.002119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.003999</td>\n",
       "      <td>0.005040</td>\n",
       "      <td>0.012799</td>\n",
       "      <td>0.004285</td>\n",
       "      <td>KNeighborsClassifier(n_neighbors=13)</td>\n",
       "      <td>{'estimator': KNeighborsClassifier(n_neighbors...</td>\n",
       "      <td>0.791429</td>\n",
       "      <td>0.105749</td>\n",
       "      <td>7</td>\n",
       "      <td>0.855238</td>\n",
       "      <td>0.005535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.165499</td>\n",
       "      <td>0.023205</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.000458</td>\n",
       "      <td>DecisionTreeClassifier()</td>\n",
       "      <td>{'estimator': DecisionTreeClassifier()}</td>\n",
       "      <td>0.705714</td>\n",
       "      <td>0.119063</td>\n",
       "      <td>8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.370283</td>\n",
       "      <td>0.099505</td>\n",
       "      <td>0.021800</td>\n",
       "      <td>0.004019</td>\n",
       "      <td>AdaBoostClassifier()</td>\n",
       "      <td>{'estimator': AdaBoostClassifier()}</td>\n",
       "      <td>0.307619</td>\n",
       "      <td>0.024112</td>\n",
       "      <td>9</td>\n",
       "      <td>0.300317</td>\n",
       "      <td>0.008762</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "3       0.658081      0.111044         0.040453        0.014389   \n",
       "5       0.106299      0.020809         0.034401        0.011316   \n",
       "0       1.255159      0.177889         0.025101        0.005320   \n",
       "8       0.194197      0.025820         0.001000        0.001095   \n",
       "2      55.622946      1.858243         0.004004        0.000770   \n",
       "6       0.913551      0.079286         0.004001        0.000448   \n",
       "7       0.003999      0.005040         0.012799        0.004285   \n",
       "4       0.165499      0.023205         0.000700        0.000458   \n",
       "1       1.370283      0.099505         0.021800        0.004019   \n",
       "\n",
       "                        param_estimator  \\\n",
       "3                ExtraTreesClassifier()   \n",
       "5                                 SVC()   \n",
       "0              RandomForestClassifier()   \n",
       "8                  LogisticRegression()   \n",
       "2          GradientBoostingClassifier()   \n",
       "6                   BaggingClassifier()   \n",
       "7  KNeighborsClassifier(n_neighbors=13)   \n",
       "4              DecisionTreeClassifier()   \n",
       "1                  AdaBoostClassifier()   \n",
       "\n",
       "                                              params  mean_test_score  \\\n",
       "3              {'estimator': ExtraTreesClassifier()}         0.837143   \n",
       "5                               {'estimator': SVC()}         0.826667   \n",
       "0            {'estimator': RandomForestClassifier()}         0.825714   \n",
       "8                {'estimator': LogisticRegression()}         0.819048   \n",
       "2        {'estimator': GradientBoostingClassifier()}         0.799048   \n",
       "6                 {'estimator': BaggingClassifier()}         0.795238   \n",
       "7  {'estimator': KNeighborsClassifier(n_neighbors...         0.791429   \n",
       "4            {'estimator': DecisionTreeClassifier()}         0.705714   \n",
       "1                {'estimator': AdaBoostClassifier()}         0.307619   \n",
       "\n",
       "   std_test_score  rank_test_score  mean_train_score  std_train_score  \n",
       "3        0.094612                1          1.000000         0.000000  \n",
       "5        0.087991                2          0.873333         0.005334  \n",
       "0        0.102402                3          1.000000         0.000000  \n",
       "8        0.097497                4          0.858519         0.007719  \n",
       "2        0.100831                5          1.000000         0.000000  \n",
       "6        0.102685                6          0.993333         0.002119  \n",
       "7        0.105749                7          0.855238         0.005535  \n",
       "4        0.119063                8          1.000000         0.000000  \n",
       "1        0.024112                9          0.300317         0.008762  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "                (\"estimator\",LogisticRegression())\n",
    "                ])\n",
    "pipe\n",
    "\n",
    "\n",
    "# Vectorise les mots avec la méthode continu bag of words\n",
    "# min_count spécifie la fréquence minimale d'un mot pour être inclus dans le modèle\n",
    "# epochs signifie que le réseau de neuronne va s'entrainer un nombre de fois sur les données \n",
    "model = Word2Vec(Lemmatise_corpus, min_count=1,hs=1,negative=0,sg=0,epochs=5)  \n",
    "\n",
    "# retoune les vecteurs de mots par document\n",
    "vectors = wvector(Lemmatise_corpus,model)\n",
    "X = vectors\n",
    "y = df_texte.label_cat1\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "              'estimator':[RandomForestClassifier(),\n",
    "                           AdaBoostClassifier(),\n",
    "                           GradientBoostingClassifier(),\n",
    "                           ExtraTreesClassifier(),\n",
    "                           DecisionTreeClassifier(),\n",
    "                           SVC(),\n",
    "                           BaggingClassifier(),\n",
    "                           KNeighborsClassifier(13),\n",
    "                           LogisticRegression()\n",
    "                          ]}\n",
    "\n",
    "\n",
    "grid = GridSearchCV(pipe,param_grid=param_grid, cv=10,return_train_score=True ,n_jobs=-1, verbose=1)\n",
    "\n",
    "grid.fit(X,y)\n",
    "\n",
    "grid_cv = pd.DataFrame(grid.cv_results_)\n",
    "\n",
    "new_cols = [i for i in grid_cv.columns if 'split' not in i.lower()]\n",
    "\n",
    "grid_cv.loc[:,new_cols].sort_values('mean_test_score', ascending=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3cb15932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 9 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dragomir\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_estimator</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.538190</td>\n",
       "      <td>0.112658</td>\n",
       "      <td>0.029913</td>\n",
       "      <td>0.013629</td>\n",
       "      <td>ExtraTreesClassifier()</td>\n",
       "      <td>{'estimator': ExtraTreesClassifier()}</td>\n",
       "      <td>0.879048</td>\n",
       "      <td>0.078078</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.137698</td>\n",
       "      <td>0.010432</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>LogisticRegression()</td>\n",
       "      <td>{'estimator': LogisticRegression()}</td>\n",
       "      <td>0.878095</td>\n",
       "      <td>0.079317</td>\n",
       "      <td>2</td>\n",
       "      <td>0.904444</td>\n",
       "      <td>0.006043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.108599</td>\n",
       "      <td>0.018558</td>\n",
       "      <td>0.033000</td>\n",
       "      <td>0.012578</td>\n",
       "      <td>SVC()</td>\n",
       "      <td>{'estimator': SVC()}</td>\n",
       "      <td>0.873333</td>\n",
       "      <td>0.088939</td>\n",
       "      <td>3</td>\n",
       "      <td>0.924339</td>\n",
       "      <td>0.005684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.078608</td>\n",
       "      <td>0.138692</td>\n",
       "      <td>0.020801</td>\n",
       "      <td>0.001165</td>\n",
       "      <td>RandomForestClassifier()</td>\n",
       "      <td>{'estimator': RandomForestClassifier()}</td>\n",
       "      <td>0.865714</td>\n",
       "      <td>0.084698</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.002199</td>\n",
       "      <td>0.000601</td>\n",
       "      <td>0.011598</td>\n",
       "      <td>0.004005</td>\n",
       "      <td>KNeighborsClassifier(n_neighbors=13)</td>\n",
       "      <td>{'estimator': KNeighborsClassifier(n_neighbors...</td>\n",
       "      <td>0.829524</td>\n",
       "      <td>0.107619</td>\n",
       "      <td>5</td>\n",
       "      <td>0.904233</td>\n",
       "      <td>0.005036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57.575393</td>\n",
       "      <td>1.448996</td>\n",
       "      <td>0.004202</td>\n",
       "      <td>0.000976</td>\n",
       "      <td>GradientBoostingClassifier()</td>\n",
       "      <td>{'estimator': GradientBoostingClassifier()}</td>\n",
       "      <td>0.823810</td>\n",
       "      <td>0.083598</td>\n",
       "      <td>6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.959500</td>\n",
       "      <td>0.087200</td>\n",
       "      <td>0.004201</td>\n",
       "      <td>0.001990</td>\n",
       "      <td>BaggingClassifier()</td>\n",
       "      <td>{'estimator': BaggingClassifier()}</td>\n",
       "      <td>0.817143</td>\n",
       "      <td>0.083658</td>\n",
       "      <td>7</td>\n",
       "      <td>0.997566</td>\n",
       "      <td>0.001500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.198097</td>\n",
       "      <td>0.042441</td>\n",
       "      <td>0.001201</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>DecisionTreeClassifier()</td>\n",
       "      <td>{'estimator': DecisionTreeClassifier()}</td>\n",
       "      <td>0.756190</td>\n",
       "      <td>0.091765</td>\n",
       "      <td>8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.273403</td>\n",
       "      <td>0.207514</td>\n",
       "      <td>0.025100</td>\n",
       "      <td>0.012293</td>\n",
       "      <td>AdaBoostClassifier()</td>\n",
       "      <td>{'estimator': AdaBoostClassifier()}</td>\n",
       "      <td>0.318095</td>\n",
       "      <td>0.039544</td>\n",
       "      <td>9</td>\n",
       "      <td>0.321376</td>\n",
       "      <td>0.018650</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "3       0.538190      0.112658         0.029913        0.013629   \n",
       "8       0.137698      0.010432         0.000800        0.000600   \n",
       "5       0.108599      0.018558         0.033000        0.012578   \n",
       "0       1.078608      0.138692         0.020801        0.001165   \n",
       "7       0.002199      0.000601         0.011598        0.004005   \n",
       "2      57.575393      1.448996         0.004202        0.000976   \n",
       "6       0.959500      0.087200         0.004201        0.001990   \n",
       "4       0.198097      0.042441         0.001201        0.000400   \n",
       "1       1.273403      0.207514         0.025100        0.012293   \n",
       "\n",
       "                        param_estimator  \\\n",
       "3                ExtraTreesClassifier()   \n",
       "8                  LogisticRegression()   \n",
       "5                                 SVC()   \n",
       "0              RandomForestClassifier()   \n",
       "7  KNeighborsClassifier(n_neighbors=13)   \n",
       "2          GradientBoostingClassifier()   \n",
       "6                   BaggingClassifier()   \n",
       "4              DecisionTreeClassifier()   \n",
       "1                  AdaBoostClassifier()   \n",
       "\n",
       "                                              params  mean_test_score  \\\n",
       "3              {'estimator': ExtraTreesClassifier()}         0.879048   \n",
       "8                {'estimator': LogisticRegression()}         0.878095   \n",
       "5                               {'estimator': SVC()}         0.873333   \n",
       "0            {'estimator': RandomForestClassifier()}         0.865714   \n",
       "7  {'estimator': KNeighborsClassifier(n_neighbors...         0.829524   \n",
       "2        {'estimator': GradientBoostingClassifier()}         0.823810   \n",
       "6                 {'estimator': BaggingClassifier()}         0.817143   \n",
       "4            {'estimator': DecisionTreeClassifier()}         0.756190   \n",
       "1                {'estimator': AdaBoostClassifier()}         0.318095   \n",
       "\n",
       "   std_test_score  rank_test_score  mean_train_score  std_train_score  \n",
       "3        0.078078                1          1.000000         0.000000  \n",
       "8        0.079317                2          0.904444         0.006043  \n",
       "5        0.088939                3          0.924339         0.005684  \n",
       "0        0.084698                4          1.000000         0.000000  \n",
       "7        0.107619                5          0.904233         0.005036  \n",
       "2        0.083598                6          1.000000         0.000000  \n",
       "6        0.083658                7          0.997566         0.001500  \n",
       "4        0.091765                8          1.000000         0.000000  \n",
       "1        0.039544                9          0.321376         0.018650  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "pipe = Pipeline([\n",
    "                (\"estimator\",LogisticRegression())\n",
    "                ])\n",
    "\n",
    "\n",
    "# sg = 1 signifie que le modèle va vectorise les mots avec la méthode skip-gramms\n",
    "# min_count spécifie la fréquence minimale d'un mot pour être inclus dans le modèle\n",
    "# epochs signifie que le réseau de neuronne va s'entrainer un nombre de fois sur les données \n",
    "model = Word2Vec(Lemmatise_corpus, min_count=1,hs=1,negative=0,sg=1,epochs=5)  \n",
    "vectors = wvector(Lemmatise_corpus,model)\n",
    "\n",
    "X = vectors\n",
    "y = df_texte.label_cat1\n",
    "\n",
    "param_grid = {\n",
    "              'estimator':[RandomForestClassifier(),\n",
    "                           AdaBoostClassifier(),\n",
    "                           GradientBoostingClassifier(),\n",
    "                           ExtraTreesClassifier(),\n",
    "                           DecisionTreeClassifier(),\n",
    "                           SVC(),\n",
    "                           BaggingClassifier(),\n",
    "                           KNeighborsClassifier(13),\n",
    "                           LogisticRegression()\n",
    "                          ]}\n",
    "\n",
    "\n",
    "grid = GridSearchCV(pipe,param_grid=param_grid, cv=10,return_train_score=True ,n_jobs=-1, verbose=1)\n",
    "\n",
    "grid.fit(X,y)\n",
    "\n",
    "grid_cv = pd.DataFrame(grid.cv_results_)\n",
    "\n",
    "new_cols = [i for i in grid_cv.columns if 'split' not in i.lower()]\n",
    "\n",
    "grid_cv.loc[:,new_cols].sort_values('mean_test_score', ascending=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9c9c4252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n",
    "logistic = LogisticRegression()\n",
    "\n",
    "model = Word2Vec(Lemmatise_corpus, min_count=1,hs=1,negative=0,sg=1,epochs=5)  \n",
    "vectors = wvector(Lemmatise_corpus,model)\n",
    "\n",
    "X = vectors\n",
    "y = df_texte.label_cat1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "logistic.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5e09df90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12, 12,  3,  4,  0,  1,  1, 12,  8,  9,  4,  8,  8,  9,  3, 12,  9,\n",
       "        1,  0,  9, 12, 10,  3,  3,  1,  8,  1, 10, 10,  8,  0, 12,  9,  8,\n",
       "        3, 10, 12, 10,  3, 12,  9, 10, 12,  8,  1,  1,  9,  4,  1, 12, 12,\n",
       "        8, 10,  1,  9,  9,  0,  9,  9,  3,  8,  8,  1, 10,  3,  4,  3,  0,\n",
       "        4,  0, 10,  8,  8,  4,  0,  9,  9,  0,  8,  9, 12, 10,  9,  9,  9,\n",
       "        9, 12,  8,  9,  8, 10,  9,  4,  8, 10,  8, 12,  9,  9, 10, 12,  9,\n",
       "        8,  1,  1,  8,  0,  1, 10,  8,  1, 10, 12, 12,  9, 12,  8,  9,  9,\n",
       "        0, 12,  3,  9, 10,  9, 10,  0,  3,  8, 12,  8,  9,  9,  4,  1,  8,\n",
       "        3,  3, 12,  0,  1,  1,  1,  9, 10,  8, 10,  1,  1, 10,  1,  8,  4,\n",
       "       12,  8,  9,  1, 10,  3,  9,  8, 12,  9,  3,  1,  9,  0,  9,  0,  9,\n",
       "        9,  9,  1, 12,  8, 10,  4,  8, 12, 10,  8,  0,  1,  9, 12,  9, 10,\n",
       "        1,  4,  1,  8,  0, 10,  9,  1,  9, 12,  8,  3,  1,  1, 10,  3,  8,\n",
       "        3, 12, 12, 10,  1,  1])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = logistic.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7f261fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12, 12,  3,  4,  0,  1,  1, 12,  8,  9,  4,  8,  8,  9,  3, 12,  9,\n",
       "        1,  0,  9, 12, 10,  3,  3,  1,  0,  1, 10, 10,  8,  0, 12,  9,  8,\n",
       "        3,  0, 12, 10,  3, 12,  9, 10, 12,  8,  1,  1,  9,  4,  1, 12, 12,\n",
       "        8, 10,  1,  9,  9,  0,  9,  9,  3,  8,  8,  1, 10,  3,  4,  3,  0,\n",
       "        4,  9, 10,  8,  0,  4,  0,  9,  0,  0, 10,  9, 12, 10,  9,  9,  9,\n",
       "        9, 12,  0,  9,  8, 10,  9,  4,  8,  0,  8, 12,  9,  9, 10, 12,  9,\n",
       "        8,  1,  0,  8,  0, 10, 10,  8,  1, 10, 12, 12,  9, 12,  8,  9,  9,\n",
       "        0, 12,  3,  9, 10,  0, 10,  0,  7,  8, 12,  8,  9,  9,  4,  1,  8,\n",
       "        3,  3, 12,  0,  1,  1,  1,  9, 10,  8, 10,  1,  1, 10,  4,  8,  4,\n",
       "       12,  0,  9,  1, 10,  3,  0,  8, 12,  9,  3,  1,  9,  8,  9,  1,  9,\n",
       "        0,  9,  1, 12,  8, 10,  4,  8, 12,  6,  8,  0,  1,  9, 12,  9,  0,\n",
       "        1,  8,  1,  8,  0, 10,  9,  1,  0, 12,  8,  3,  1,  0, 10,  3,  8,\n",
       "        3, 12, 12, 10,  1,  1])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "70c70505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.48      0.60        27\n",
      "           1       0.88      0.97      0.92        29\n",
      "           3       0.94      1.00      0.97        17\n",
      "           4       0.91      0.91      0.91        11\n",
      "           6       0.00      0.00      0.00         1\n",
      "           7       0.00      0.00      0.00         1\n",
      "           8       0.85      0.94      0.89        31\n",
      "           9       0.88      0.97      0.93        39\n",
      "          10       0.85      0.92      0.88        25\n",
      "          12       1.00      1.00      1.00        29\n",
      "\n",
      "    accuracy                           0.89       210\n",
      "   macro avg       0.71      0.72      0.71       210\n",
      "weighted avg       0.88      0.89      0.88       210\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dragomir\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Dragomir\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Dragomir\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6798b7",
   "metadata": {},
   "source": [
    "### Classification de texte avec BERT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a00d3ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  2620\n"
     ]
    }
   ],
   "source": [
    "tokens = []\n",
    "_ = [tokens.append(str(txt).replace(\"'\", '').replace(\",\",\"\").replace(\"[\",'').replace(\"]\",'')) for txt in Lemmatise_corpus]\n",
    "print('Max sentence length: ', max([len(sen) for sen in tokens]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f6a6fbd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([9, 0, 0,  ..., 0, 0, 0])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prétraitement des textes et des étiquettes\n",
    "# utiliser la CPU \n",
    "device = torch.device(\"cpu\" if torch.cpu else \"cuda\")\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels = 13)\n",
    "\n",
    "model.to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "inputs = tokenizer(tokens,\n",
    "                   padding=True,\n",
    "                   truncation=True,\n",
    "                   max_length= 100,\n",
    "                   return_tensors=\"pt\")\n",
    "\n",
    "labels = torch.tensor(df_texte.label_cat1 , dtype=torch.long)\n",
    "\n",
    "inputs.to(device)\n",
    "\n",
    "labels.to(device)\n",
    "# Entraîner le modèle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c28b3b53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  3145,  3444,  ...,  4408,  3103,   102],\n",
       "        [  101, 12827,  2938,  ...,     0,     0,     0],\n",
       "        [  101,  3145,  3444,  ...,  2300,  3707,   102],\n",
       "        ...,\n",
       "        [  101,  4965, 19169,  ...,     0,     0,     0],\n",
       "        [  101,  4965,  2813,  ...,     0,     0,     0],\n",
       "        [  101,  4965, 19169,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "67b849fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(inputs['input_ids'], labels, \n",
    "                                                            random_state=2018, test_size=0.2)\n",
    "# Performing same steps on the attention masks\n",
    "train_masks, validation_masks, _, _ = train_test_split(inputs['attention_mask'], labels,\n",
    "                                             random_state=2018, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ba481eb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(840, 840, 840, 210, 210, 210)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_inputs), len(train_masks), len(train_labels), len(validation_inputs), len(validation_masks[:840]), len(validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7ae3fdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#Creating the DataLoader which will help us to load data into the GPU/CPU\n",
    "batch_size = 20\n",
    "\n",
    "# Create the DataLoader for our training set.\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set.\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "248cc447",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dragomir\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# AdamW is an optimizer which is a Adam Optimzier with weight-decay-fix\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, \n",
    "                  eps = 1e-8 \n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e241805f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.optim.lr_scheduler.LambdaLR at 0x1fbc68b6b50>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 2\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "print(len(train_dataloader))\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n",
    "scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7cb2e87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a6f50b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the helper function to have a watch on elapsed time\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "916e15a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 3 ========\n",
      "Training...\n",
      "  Batch    10  of     42.    Elapsed: 0:01:17.\n",
      "  Batch    20  of     42.    Elapsed: 0:02:32.\n",
      "  Batch    30  of     42.    Elapsed: 0:03:50.\n",
      "  Batch    40  of     42.    Elapsed: 0:05:06.\n",
      "\n",
      "  Average training loss: 2.11\n",
      "  Training epoch took: 0:05:13\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.66\n",
      "  Validation took: 0:00:27\n",
      "\n",
      "======== Epoch 2 / 3 ========\n",
      "Training...\n",
      "  Batch    10  of     42.    Elapsed: 0:01:16.\n",
      "  Batch    20  of     42.    Elapsed: 0:02:32.\n",
      "  Batch    30  of     42.    Elapsed: 0:03:49.\n",
      "  Batch    40  of     42.    Elapsed: 0:05:05.\n",
      "\n",
      "  Average training loss: 1.58\n",
      "  Training epoch took: 0:05:12\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.69\n",
      "  Validation took: 0:00:27\n",
      "\n",
      "======== Epoch 3 / 3 ========\n",
      "Training...\n",
      "  Batch    10  of     42.    Elapsed: 0:01:17.\n",
      "  Batch    20  of     42.    Elapsed: 0:02:33.\n",
      "  Batch    30  of     42.    Elapsed: 0:03:49.\n",
      "  Batch    40  of     42.    Elapsed: 0:05:06.\n",
      "\n",
      "  Average training loss: 1.48\n",
      "  Training epoch took: 0:05:13\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.69\n",
      "  Validation took: 0:00:28\n"
     ]
    }
   ],
   "source": [
    "# Store the average loss after each epoch so we can plot them.\n",
    "loss_values = []\n",
    "epochs = 3\n",
    "for epoch in range(0,epochs):\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch + 1, epochs))\n",
    "    print('Training...')\n",
    "    \n",
    "    t0 = time.time()\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        b_input_ids = batch[0]\n",
    "        b_input_mask = batch[1]\n",
    "        b_labels = batch[2]\n",
    "        \n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()  \n",
    "        # Progress update every 40 batches.\n",
    "        if step % 10 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(input_ids = b_input_ids ,attention_mask = b_input_mask, labels=b_labels)\n",
    "        \n",
    "        # The call to `model` always returns a tuple, so we need to pull the \n",
    "        # loss value out of the tuple.\n",
    "        loss = outputs[0]\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Calculate the average loss over the training data.\n",
    "        avg_train_loss = total_loss / len(train_dataloader)            \n",
    "\n",
    "        # Store the loss value for plotting the learning curve.\n",
    "        loss_values.append(avg_train_loss)\n",
    "        if step == 40:\n",
    "            print(\"\")\n",
    "            print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "            print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "            print(\"\")\n",
    "            print(\"Running Validation...\")\n",
    "        \n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        \n",
    "        batch = tuple(t for t in batch)\n",
    "        \n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # Telling the model not to compute or store gradients, saving memory and\n",
    "        # speeding up validation\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # This will return the logits rather than the loss because we have\n",
    "            # not provided labels.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            outputs = model(b_input_ids,\n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "        # Put the model in evaluation mode--the dropout layers behave differently\n",
    "        # during evaluation.\n",
    "        logits = outputs[0]\n",
    "        \n",
    "        # Calculate the accuracy for this batch of test sentences.\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, b_labels)\n",
    "        \n",
    "        # Accumulate the total accuracy.\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "        # Track the number of batches\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4a9c6a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.06276288486662365,\n",
       " 0.12530690147763207,\n",
       " 0.1868147679737636,\n",
       " 0.24321325052352177,\n",
       " 0.29947900204431444,\n",
       " 0.35791354519980295,\n",
       " 0.4151692731039865,\n",
       " 0.4740180742172968,\n",
       " 0.5299875793002901,\n",
       " 0.588047464688619,\n",
       " 0.6437131904420399,\n",
       " 0.699635142371768,\n",
       " 0.7501753795714605,\n",
       " 0.8038823434284755,\n",
       " 0.8553483202343896,\n",
       " 0.9088643164861769,\n",
       " 0.9607306434994652,\n",
       " 1.013925035794576,\n",
       " 1.0660182237625122,\n",
       " 1.120287231036595,\n",
       " 1.1704101789565313,\n",
       " 1.2206562019529796,\n",
       " 1.2714766263961792,\n",
       " 1.318939467271169,\n",
       " 1.3677479795047216,\n",
       " 1.4150880773862202,\n",
       " 1.4587522205852328,\n",
       " 1.5057093132109869,\n",
       " 1.5542563994725545,\n",
       " 1.6015789310137432,\n",
       " 1.6510472212518965,\n",
       " 1.6983090184983753,\n",
       " 1.7464575256620134,\n",
       " 1.7906955991472517,\n",
       " 1.8333004258927845,\n",
       " 1.8817569812138875,\n",
       " 1.9300561916260492,\n",
       " 1.9740923614729018,\n",
       " 2.016504148642222,\n",
       " 2.063247186797006,\n",
       " 2.1074296094122387,\n",
       " 2.1479559938112893,\n",
       " 0.045001194590613955,\n",
       " 0.0875028343427749,\n",
       " 0.12814861536026,\n",
       " 0.17126727388018653,\n",
       " 0.2110118241537185,\n",
       " 0.25090584584644865,\n",
       " 0.2918299323036557,\n",
       " 0.3327109898839678,\n",
       " 0.371062057358878,\n",
       " 0.410775933946882,\n",
       " 0.45193376143773395,\n",
       " 0.4930283966518584,\n",
       " 0.5357786956287566,\n",
       " 0.5716578023774284,\n",
       " 0.6144092735790071,\n",
       " 0.6520879382178897,\n",
       " 0.6933361149969555,\n",
       " 0.7326129930359977,\n",
       " 0.7703452819869632,\n",
       " 0.8065206408500671,\n",
       " 0.843392136551085,\n",
       " 0.876893142859141,\n",
       " 0.913094489347367,\n",
       " 0.9472831686337789,\n",
       " 0.9864124201592945,\n",
       " 1.0235178470611572,\n",
       " 1.063762250400725,\n",
       " 1.1026179847263156,\n",
       " 1.1375195724623544,\n",
       " 1.1738724055744352,\n",
       " 1.2078169215293157,\n",
       " 1.2420426124618167,\n",
       " 1.2757158279418945,\n",
       " 1.3159364688964117,\n",
       " 1.353613413515545,\n",
       " 1.3911534008525668,\n",
       " 1.4256201954115004,\n",
       " 1.4635354166939145,\n",
       " 1.498678397564661,\n",
       " 1.5380637390272958,\n",
       " 1.5762322090920948,\n",
       " 1.6154203897430783,\n",
       " 0.03549844310397193,\n",
       " 0.06869522162846156,\n",
       " 0.10946392729168847,\n",
       " 0.1487126974832444,\n",
       " 0.18289202451705933,\n",
       " 0.2173047207650684,\n",
       " 0.2584833644685291,\n",
       " 0.2915813071387155,\n",
       " 0.32549171788351877,\n",
       " 0.35999146813438054,\n",
       " 0.3954369936670576,\n",
       " 0.43027541750953313,\n",
       " 0.46687970274970647,\n",
       " 0.5010432629358201,\n",
       " 0.5364058982758295,\n",
       " 0.5733832858857655,\n",
       " 0.6083224359012785,\n",
       " 0.6446567291305179,\n",
       " 0.688752639861334,\n",
       " 0.7234730975968497,\n",
       " 0.7611310254959833,\n",
       " 0.7957739432652792,\n",
       " 0.8307762032463437,\n",
       " 0.868358333905538,\n",
       " 0.9004855638458615,\n",
       " 0.9348598094213576,\n",
       " 0.9694125368481591,\n",
       " 1.0115058450471788,\n",
       " 1.0491303347405934,\n",
       " 1.086703964642116,\n",
       " 1.1246656292960757,\n",
       " 1.1596505216189794,\n",
       " 1.196408110005515,\n",
       " 1.2346279166993641,\n",
       " 1.2705349354516893,\n",
       " 1.3072045814423334,\n",
       " 1.3422126571337383,\n",
       " 1.3750178047588892,\n",
       " 1.410267023813157,\n",
       " 1.442773943855649,\n",
       " 1.4793424663089572,\n",
       " 1.516076425711314]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ada9e171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classe du texte à prédire\n"
     ]
    }
   ],
   "source": [
    "print('classe du texte à prédire'.format(labels[12]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e05352f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bert():\n",
    "    model.eval()\n",
    "    y_pred  = []\n",
    "    # Tracking variables \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        batch = tuple(t for t in batch)\n",
    "        \n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # Telling the model not to compute or store gradients, saving memory and\n",
    "        # speeding up validation\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # This will return the logits rather than the loss because we have\n",
    "            # not provided labels.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            outputs = model(b_input_ids,\n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask,\n",
    "                            output_hidden_states = True)\n",
    "        # Put the model in evaluation mode--the dropout layers behave differently\n",
    "        # during evaluation.\n",
    "        logits = outputs[0]\n",
    "        out = outputs\n",
    "        \n",
    "        #récupère les prédictions\n",
    "        y_pred.append(logits)\n",
    "        \n",
    "        # Calculate the accuracy for this batch of test sentences.\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, b_labels)\n",
    "        \n",
    "        # Accumulate the total accuracy.\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "        # Track the number of batches\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "    return y_pred,out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5fc89bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.69\n",
      "  Validation took: 0:18:54\n"
     ]
    }
   ],
   "source": [
    "pred, out  = eval_bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d19d8a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = []\n",
    "for nb, classe in enumerate(pred):\n",
    "    for i in range(len(pred[nb])):\n",
    "        predicted_class = torch.argmax(pred[nb][i]).item()\n",
    "        pred_labels.append(predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a4f740bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out.hidden_states[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0ecc8843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8, 12,  9, 12,  9,  0,  0, 10,  3,  1,  9, 10,  1,  8,  3,  8,  8,\n",
       "        3,  0,  3, 12,  0,  9,  9,  1,  8,  8,  8,  9, 12,  8,  9,  9,  3,\n",
       "        9, 10,  1,  3,  8,  8, 10,  9,  8,  8,  8,  8,  0, 12,  1,  0, 10,\n",
       "        9,  9, 12,  3, 10,  1,  0,  0,  8,  9,  9,  3,  8,  1,  0,  3,  3,\n",
       "       12,  3,  9,  8,  9,  9,  1,  0,  9, 10,  9, 12, 12, 10,  0, 10,  0,\n",
       "        8,  0,  8,  3,  3,  8,  3,  9,  9,  3,  3,  1,  3,  3,  8, 12, 10,\n",
       "        9,  9,  0,  9,  1,  3, 12,  9,  1,  8,  3, 10, 12,  1, 12,  3,  9,\n",
       "        1,  3,  9,  0, 10,  1,  8,  1,  9,  3,  0,  3,  0, 12,  8,  8,  1,\n",
       "       12, 10, 10,  0,  9, 10,  3,  3,  0, 12,  0,  9,  8,  8,  0,  0,  3,\n",
       "        0,  0,  0,  8,  9, 12, 10, 12, 12,  8,  1,  0,  9,  3, 10,  0,  8,\n",
       "        8, 12,  0,  1, 12,  9,  8,  3, 12,  8, 10,  8,  8,  0,  1,  3, 12,\n",
       "       10,  9,  3,  0,  8,  3,  8,  1,  0,  9, 12,  3,  3,  9,  3,  0,  0,\n",
       "        9,  0,  0,  8,  9,  1])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bb97ce62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8, 12,  8, 12,  9,  9,  9,  8,  3,  1,  9, 10,  1,  8,  3,  8,  8,  3,\n",
       "        10,  3, 12,  0,  9,  9,  1, 10, 10,  8,  8, 12,  8,  0,  9,  3,  9, 10,\n",
       "         1,  4, 10, 10, 10,  8,  8,  8,  8, 10,  0, 12,  1,  0, 10,  9,  9, 12,\n",
       "         3, 10,  1,  0,  1,  8,  8,  0,  3,  0,  1,  0,  3,  0, 12,  3,  9,  1,\n",
       "         0,  9,  1,  0,  0, 10,  0, 12, 12, 10,  0, 10,  0, 10,  1,  8,  4,  4,\n",
       "         8,  3,  9,  9,  3,  3,  1,  3,  4,  8, 12,  9,  9,  9,  0,  9,  1,  4,\n",
       "        12,  9,  1,  8,  4,  0, 12,  1, 12,  4,  9,  1,  3,  9,  0, 10,  1, 10,\n",
       "         1,  9,  4,  0,  3,  0, 12,  8,  8,  8, 12, 10, 10,  0,  9, 10,  3,  2,\n",
       "         1, 12,  1,  9, 10,  8,  0,  0,  4,  9,  9,  0,  8,  9, 12, 10, 12, 12,\n",
       "        10,  1,  9,  9,  3, 10,  1, 10,  8, 12,  0,  1, 12,  9, 10, 10, 12, 10,\n",
       "        10,  8,  8,  0,  1,  7, 12, 12,  9,  4,  0,  8,  8,  8,  4,  9,  9, 12,\n",
       "         3,  3, 10,  3,  1,  0,  0,  0,  9, 10,  9,  0])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "36dadc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.68      0.64        31\n",
      "           1       0.86      0.72      0.78        25\n",
      "           2       0.00      0.00      0.00         1\n",
      "           3       0.57      1.00      0.73        20\n",
      "           4       0.00      0.00      0.00        11\n",
      "           7       0.00      0.00      0.00         1\n",
      "           8       0.61      0.77      0.68        30\n",
      "           9       0.71      0.77      0.74        35\n",
      "          10       0.79      0.48      0.60        31\n",
      "          12       1.00      0.96      0.98        25\n",
      "\n",
      "    accuracy                           0.70       210\n",
      "   macro avg       0.51      0.54      0.51       210\n",
      "weighted avg       0.69      0.70      0.68       210\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dragomir\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Dragomir\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Dragomir\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(validation_labels,pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afc13c4",
   "metadata": {},
   "source": [
    "### Classification de texte avec Universal sentence encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "806fa5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.05380953 -0.05427526  0.03763935 ...  0.04525556  0.0400887\n",
      "  -0.04485746]\n",
      " [-0.0504561  -0.04426588  0.0262354  ...  0.02325921  0.02831347\n",
      "   0.03676867]\n",
      " [-0.05464859 -0.05145617 -0.02325417 ...  0.05250543 -0.03077259\n",
      "  -0.04503982]\n",
      " ...\n",
      " [-0.01919353 -0.04591073 -0.02684828 ... -0.02107586  0.04736629\n",
      "  -0.0543308 ]\n",
      " [-0.0112249  -0.04495183  0.03897667 ...  0.01591754 -0.01513253\n",
      "  -0.00175135]\n",
      " [-0.05246769 -0.04012159  0.03473873 ...  0.02537613 -0.01057997\n",
      "   0.002412  ]], shape=(1050, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "embeddings = embed(tokens)\n",
    "\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0ff039fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1050, 512), dtype=float32, numpy=\n",
       "array([[-0.05380953, -0.05427526,  0.03763935, ...,  0.04525556,\n",
       "         0.0400887 , -0.04485746],\n",
       "       [-0.0504561 , -0.04426588,  0.0262354 , ...,  0.02325921,\n",
       "         0.02831347,  0.03676867],\n",
       "       [-0.05464859, -0.05145617, -0.02325417, ...,  0.05250543,\n",
       "        -0.03077259, -0.04503982],\n",
       "       ...,\n",
       "       [-0.01919353, -0.04591073, -0.02684828, ..., -0.02107586,\n",
       "         0.04736629, -0.0543308 ],\n",
       "       [-0.0112249 , -0.04495183,  0.03897667, ...,  0.01591754,\n",
       "        -0.01513253, -0.00175135],\n",
       "       [-0.05246769, -0.04012159,  0.03473873, ...,  0.02537613,\n",
       "        -0.01057997,  0.002412  ]], dtype=float32)>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f75ce466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.05380953, -0.05427526,  0.03763935, ...,  0.04525556,\n",
       "         0.0400887 , -0.04485746],\n",
       "       [-0.0504561 , -0.04426588,  0.0262354 , ...,  0.02325921,\n",
       "         0.02831347,  0.03676867],\n",
       "       [-0.05464859, -0.05145617, -0.02325417, ...,  0.05250543,\n",
       "        -0.03077259, -0.04503982],\n",
       "       ...,\n",
       "       [-0.01919353, -0.04591073, -0.02684828, ..., -0.02107586,\n",
       "         0.04736629, -0.0543308 ],\n",
       "       [-0.0112249 , -0.04495183,  0.03897667, ...,  0.01591754,\n",
       "        -0.01513253, -0.00175135],\n",
       "       [-0.05246769, -0.04012159,  0.03473873, ...,  0.02537613,\n",
       "        -0.01057997,  0.002412  ]], dtype=float32)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape\n",
    "embeddings.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ea572771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 9 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dragomir\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_estimator</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.546545</td>\n",
       "      <td>0.078662</td>\n",
       "      <td>0.034702</td>\n",
       "      <td>0.010478</td>\n",
       "      <td>ExtraTreesClassifier()</td>\n",
       "      <td>{'estimator': ExtraTreesClassifier()}</td>\n",
       "      <td>0.837143</td>\n",
       "      <td>0.091493</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.079431</td>\n",
       "      <td>0.095665</td>\n",
       "      <td>0.026701</td>\n",
       "      <td>0.007825</td>\n",
       "      <td>RandomForestClassifier()</td>\n",
       "      <td>{'estimator': RandomForestClassifier()}</td>\n",
       "      <td>0.826667</td>\n",
       "      <td>0.105607</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.094993</td>\n",
       "      <td>0.013918</td>\n",
       "      <td>0.026303</td>\n",
       "      <td>0.007667</td>\n",
       "      <td>SVC()</td>\n",
       "      <td>{'estimator': SVC()}</td>\n",
       "      <td>0.824762</td>\n",
       "      <td>0.088032</td>\n",
       "      <td>3</td>\n",
       "      <td>0.868360</td>\n",
       "      <td>0.005838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.158104</td>\n",
       "      <td>0.022050</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>LogisticRegression()</td>\n",
       "      <td>{'estimator': LogisticRegression()}</td>\n",
       "      <td>0.824762</td>\n",
       "      <td>0.097329</td>\n",
       "      <td>3</td>\n",
       "      <td>0.853228</td>\n",
       "      <td>0.008598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53.011235</td>\n",
       "      <td>1.596420</td>\n",
       "      <td>0.004398</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>GradientBoostingClassifier()</td>\n",
       "      <td>{'estimator': GradientBoostingClassifier()}</td>\n",
       "      <td>0.796190</td>\n",
       "      <td>0.103735</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.826296</td>\n",
       "      <td>0.080196</td>\n",
       "      <td>0.003501</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>BaggingClassifier()</td>\n",
       "      <td>{'estimator': BaggingClassifier()}</td>\n",
       "      <td>0.792381</td>\n",
       "      <td>0.110065</td>\n",
       "      <td>6</td>\n",
       "      <td>0.994497</td>\n",
       "      <td>0.002495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.000502</td>\n",
       "      <td>0.009898</td>\n",
       "      <td>0.002073</td>\n",
       "      <td>KNeighborsClassifier(n_neighbors=13)</td>\n",
       "      <td>{'estimator': KNeighborsClassifier(n_neighbors...</td>\n",
       "      <td>0.788571</td>\n",
       "      <td>0.107226</td>\n",
       "      <td>7</td>\n",
       "      <td>0.849524</td>\n",
       "      <td>0.007628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.189835</td>\n",
       "      <td>0.027661</td>\n",
       "      <td>0.000801</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>DecisionTreeClassifier()</td>\n",
       "      <td>{'estimator': DecisionTreeClassifier()}</td>\n",
       "      <td>0.748571</td>\n",
       "      <td>0.104084</td>\n",
       "      <td>8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.320115</td>\n",
       "      <td>0.222486</td>\n",
       "      <td>0.018453</td>\n",
       "      <td>0.003195</td>\n",
       "      <td>AdaBoostClassifier()</td>\n",
       "      <td>{'estimator': AdaBoostClassifier()}</td>\n",
       "      <td>0.290476</td>\n",
       "      <td>0.039784</td>\n",
       "      <td>9</td>\n",
       "      <td>0.296402</td>\n",
       "      <td>0.017020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "3       0.546545      0.078662         0.034702        0.010478   \n",
       "0       1.079431      0.095665         0.026701        0.007825   \n",
       "5       0.094993      0.013918         0.026303        0.007667   \n",
       "8       0.158104      0.022050         0.000500        0.000500   \n",
       "2      53.011235      1.596420         0.004398        0.000663   \n",
       "6       0.826296      0.080196         0.003501        0.000500   \n",
       "7       0.002500      0.000502         0.009898        0.002073   \n",
       "4       0.189835      0.027661         0.000801        0.000401   \n",
       "1       1.320115      0.222486         0.018453        0.003195   \n",
       "\n",
       "                        param_estimator  \\\n",
       "3                ExtraTreesClassifier()   \n",
       "0              RandomForestClassifier()   \n",
       "5                                 SVC()   \n",
       "8                  LogisticRegression()   \n",
       "2          GradientBoostingClassifier()   \n",
       "6                   BaggingClassifier()   \n",
       "7  KNeighborsClassifier(n_neighbors=13)   \n",
       "4              DecisionTreeClassifier()   \n",
       "1                  AdaBoostClassifier()   \n",
       "\n",
       "                                              params  mean_test_score  \\\n",
       "3              {'estimator': ExtraTreesClassifier()}         0.837143   \n",
       "0            {'estimator': RandomForestClassifier()}         0.826667   \n",
       "5                               {'estimator': SVC()}         0.824762   \n",
       "8                {'estimator': LogisticRegression()}         0.824762   \n",
       "2        {'estimator': GradientBoostingClassifier()}         0.796190   \n",
       "6                 {'estimator': BaggingClassifier()}         0.792381   \n",
       "7  {'estimator': KNeighborsClassifier(n_neighbors...         0.788571   \n",
       "4            {'estimator': DecisionTreeClassifier()}         0.748571   \n",
       "1                {'estimator': AdaBoostClassifier()}         0.290476   \n",
       "\n",
       "   std_test_score  rank_test_score  mean_train_score  std_train_score  \n",
       "3        0.091493                1          1.000000         0.000000  \n",
       "0        0.105607                2          1.000000         0.000000  \n",
       "5        0.088032                3          0.868360         0.005838  \n",
       "8        0.097329                3          0.853228         0.008598  \n",
       "2        0.103735                5          1.000000         0.000000  \n",
       "6        0.110065                6          0.994497         0.002495  \n",
       "7        0.107226                7          0.849524         0.007628  \n",
       "4        0.104084                8          1.000000         0.000000  \n",
       "1        0.039784                9          0.296402         0.017020  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "                (\"estimator\",LogisticRegression())\n",
    "                ])\n",
    "pipe\n",
    "\n",
    "X = embed\n",
    "y = df_texte.label_cat1\n",
    "\n",
    "param_grid = {\n",
    "              'estimator':[RandomForestClassifier(),\n",
    "                           AdaBoostClassifier(),\n",
    "                           GradientBoostingClassifier(),\n",
    "                           ExtraTreesClassifier(),\n",
    "                           DecisionTreeClassifier(),\n",
    "                           SVC(),\n",
    "                           BaggingClassifier(),\n",
    "                           KNeighborsClassifier(13),\n",
    "                           LogisticRegression()\n",
    "                          ]}\n",
    "\n",
    "\n",
    "grid = GridSearchCV(pipe,param_grid=param_grid, cv=10,return_train_score=True ,n_jobs=-1, verbose=1)\n",
    "\n",
    "grid.fit(document_vectors,y)\n",
    "\n",
    "grid_cv = pd.DataFrame(grid.cv_results_)\n",
    "\n",
    "new_cols = [i for i in grid_cv.columns if 'split' not in i.lower()]\n",
    "\n",
    "grid_cv.loc[:,new_cols].sort_values('mean_test_score', ascending=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b9026559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(random_state=0)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "\n",
    "\n",
    "X = embeddings.numpy()\n",
    "y = df_texte.label_cat1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "clf = ExtraTreesClassifier(n_estimators=100, random_state=0)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "59c105ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8952380952380953"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9711907f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.74      0.81        31\n",
      "           1       0.92      0.88      0.90        40\n",
      "           2       0.00      0.00      0.00         1\n",
      "           3       0.96      1.00      0.98        22\n",
      "           4       0.88      1.00      0.93         7\n",
      "           5       0.00      0.00      0.00         1\n",
      "           8       0.76      1.00      0.86        22\n",
      "           9       0.86      0.94      0.90        34\n",
      "          10       0.92      0.81      0.86        27\n",
      "          12       1.00      1.00      1.00        25\n",
      "\n",
      "    accuracy                           0.90       210\n",
      "   macro avg       0.72      0.74      0.72       210\n",
      "weighted avg       0.89      0.90      0.89       210\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dragomir\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Dragomir\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Dragomir\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c7c42dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  6,  8,  9, 10, 12])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(y_test.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6ef74085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 10,  9,  1,  1,  8,  3,  3,  1,  1,  8,  1,  3,  8,  1,  0,  0,\n",
       "        0,  9,  9,  9,  2,  0,  3, 10, 10,  3,  0, 12,  9,  1,  9,  8,  8,\n",
       "        9, 12,  3,  8,  0,  8,  0,  1,  0, 12,  0,  0, 10,  8, 10, 12, 12,\n",
       "       10,  6,  3,  1, 12, 10,  1, 12,  0,  1,  4,  9,  8,  3, 10,  1, 12,\n",
       "        0,  0,  9,  9,  9, 12, 12,  1,  1,  9,  3,  8,  0,  0,  9,  1, 10,\n",
       "        9,  0,  3,  9,  3,  3,  8, 10,  8,  1,  3,  0,  0,  1,  9,  8,  8,\n",
       "        8,  9,  0,  9, 12, 10, 10,  8,  8,  8,  9,  1,  3,  0,  1, 10,  9,\n",
       "        4,  8,  1, 12, 10,  9,  6,  8, 10,  0,  1, 12, 12,  4,  0, 12, 10,\n",
       "        8,  9,  9, 12,  3,  0,  8, 10,  8,  2,  0,  1,  8,  8, 12,  8,  3,\n",
       "        3,  0,  0,  0,  9, 12,  8,  9,  8,  8,  4,  8, 12,  0,  1, 12, 10,\n",
       "        8, 10, 12,  9,  0,  0,  3, 12, 12, 10,  1,  9,  1, 10, 10,  8,  1,\n",
       "        9,  0, 12,  8, 10,  9, 10,  3,  1,  0,  1, 12,  0, 10,  4, 12, 12,\n",
       "        0, 10,  0, 12,  8,  9])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "69d1745c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10,  9,  1,  3,  8,  0, 12,  4])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_pred).unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
